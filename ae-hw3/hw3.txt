/===================================================\
| Algorithm Engineering: Homework 3                 |
| By: Steven Kneiser                                |
|                                                   |
| Code avialable on my GitHub:                      |
|   github.com/theshteves/school/tree/master/ae-hw3 |
\===================================================/


[1] (Quicksort vs. Insertion Sort)
   HYPOTHESIS:
     While I expect Quicksort to dominate, I've read that Insertion Sort works
     better for values around 20 (Numerical Recipes in C) which makes sense
     since I've heard that Insertion Sort is best on nearly-sorted data which
     would apply with small values given the minimal number of swaps required.

   METHODS:
     Borrowing source code from the sorting section of *Numerical Recipes
     in C,* Quicksort & Insertion Sort where easily done (except for
     some tweaking for my uses).  I took CLI-arguments: N (array size)
     and K (repititions).  I would randomly populate an array of size N,
     then copy all those values over to a second array so each algorithm
     was working with the exact same data (0-to-1 decimals).  Lastly,
     I would log their run-times, re-populate them, sort, and repeat K-times.
     All results for a given N are in microseconds averaged over 10 runs.

   RESULTS:       N |  5 | 10 | 15 | 20 | 30 | 40 | 50 | 60 | 70 |
     =============================================================
     Insertion Sort |  1 | 1.4| 1.7| 2.3| 2.8| 5.4| 6.8| 8.8|16.7|
          Quicksort |  2 | 1.4| 2.3| 3.2| 4.0| 4.9| 6.3| 7.1|12.4|

   DISCUSSION:
     Just looking at the results, I regret not writing the code to just store
     and average MANY more simulations for me because the deviation seemed
     too strong.  Also I would like to have found a better timing method:
     in terms of the efficiency bonus, the bonus is at such small numbers
     that I don't even trust my own numbers since the module would be returning
     mere 1's or 2's at N < 15 so there's not much accuracy.

   CONCLUSIONS:
     From my results, I'm led to believe that my implementation of
     Insertion Sort semi-consistently outperforms my equivalent of Quicksort
     for N between roughly 15 to 35.  Anything smaller is an equal race
     while anything greater than about 35 begins to quickly favor Quicksort.



[2] (Hybrid Sorting)
   HYPOTHESIS:
     I expect to take advantage of the threshold from experiment 1 in order
     to simply outperform the less-ideal runtimes of both.

   METHODS:
     My methods are the exact same as the previous experiment.  In fact, I
     actually copied the array twice over and clocked my hybrid sorting
     algorithm alongside the other two.  My hybrid sorting algorithm is
     unfortunately a crude, one-liner, ternary conditional operator
     that uses Insertion Sort for everything under 35 and Quicksort
     for everything else.

   RESULTS:       N |  5 | 10 | 15 | 20 | 30 | 40 | 50 | 60 | 70 |
     =============================================================
     Insertion Sort |  1 | 1.4| 1.7| 2.3| 2.8| 5.4| 6.8| 8.8|16.7|
          Quicksort |  2 | 1.4| 2.3| 3.2| 4.0| 4.9| 6.3| 7.1|12.4|
        Hybrid Sort | 0.5| 1.3| 1.7| 2.1| 2.4| 5.0| 6.1| 7.5|12.1|

   DISCUSSION:
     I REALLY wish I had some more time to explore this further.  I would love
     to think of more sophisticated models to switch between many sorts.
     These results threw me for a loop for a while as I was convinced
     something was off somewhere.  At first, there was! (I was calling the
     hybrid sort on an array that another already sorted.  If I were to do it
     again, I would make all of the improvements as suggested in the
     previous experiment as well as brainstorm a more sophisticated sort.

   CONCLUSIONS:
     My crude hybird sort successfully performed as well as, if not better,
     than the other two on average. 



[3] (Binary Search Trees vs. Hash Tables)
   HYPOTHESIS:
     Given my understanding that Hash Table insertion is O(1) whereas
     for BST's it is O(log(n)), I expect the Hash Table to outperform in
     accordance with those complexities.  The BST MIGHT overperform for
     trivially small N like Insertion Sort but even that I doubt.

   METHODS:
     Per the assignment suggestion, I studied the insertion time-complexity of
     multiset & unordered_multiset as if they were accurate representations of
     a hash table and binary search tree, respectively.  Taking advantage of
     the same CLI arguments as before, N & K, I simply generate a random value
     N times and time how long each data structure took to insert.  I add this
     timedelta to their total times which I keep track of & repeat K times.

   RESULTS:           N | 10 | 50  | 100 | 500 | 1000 | 5000  | 10000  |
     ===================================================================
     unordered_multiset | 25 | 92  | 171 | 582 | 1557 | 6275  | 13012  |
               multiset | 34 | 340 | 1049|15028| 69330|1340124| 5401004|

   DISCUSSION:
     I'm concerned that I made a serious error somewhere because the result
     is REALLY counter-intuitive to me.  I am almost so sure of my hypothesis
     that I would much rather figure out any "errors" in my code rather than
     sit down and think about what I just witnessed.  Regardless, I learned
     about the insertion complexity of these two contains side-by-side.

   CONCLUSIONS:
     Despite my hypothesis, unordered_multiset's insertion is MUCH better
     then that of multisets.  And in the twisted analogy with C++'s STL (which
     is entirely to thank for these containers), a "BST"'s insertion is better
     than that of a "Hash Table." If I didn't make a complete eror somewhere,
     then these results suggest that a Hash Table's insertion is not as
     performant as that of a BST.



[4] (... vs. Vectors)
   HYPOTHESIS:
     I expect simple algorithms to leave the BST to outperform the vector
     on this one, like O(log(n)) to O(n) behavior although I have NO idea how
     a vector is implemented in the STL which I hold very high regard for,
     so I wouldn't be surprised if it was an even more optimized structure.

   METHODS:
     My methods for this one closely mirror the previous experiment with
     the exception of an included "sorted-vector."  I didn't even need
     to write my own data structure as much as make clever use of C++'s STL.
     As long as you always insert into sorted order then it yields a
     sorted vector.  I got some inspiration for this solution while browsing
     C++'s algorithm library whose use I am particularly proud of.
     Time it alongside the multiset's and you get:

   RESULTS:           N | 10 | 50  | 100 | 500 | 1000 | 5000  | 10000  |
     ===================================================================
     unordered_multiset | 25 | 92  | 171 | 582 | 1557 | 6275  | 13012  |
               multiset | 34 | 340 | 1049|15028| 69330|1340124| 5401004|
          sorted-vector | 15 | 63  | 118 | 467 | 1018 | 4225  |  8822  |

   DISCUSSION:
     I was really impressed at the sorted-vector's performance given the
     pessimistic mention in the problem satement.  However, the more I look
     at the code, the less I am surprised.  I have strong trust in the
     tuned performance of STL structures and their algorithms, so the thought
     that a clever use of the C++ algorithm library with one of the most
     common templates is quite performant is not surprising at all..

   CONCLUSIONS:
     My hypothesis was quite off: the vector's highly-tuned implementation
     is so great that just a clever use of iterators is enough to compete
     with data structures more targeted towards preserving order such as a BST.
